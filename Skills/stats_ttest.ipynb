{
"cells": [
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"deletable": false,
"editable": false
},
"outputs": [],
"source": [
"# Initialize Otter\n",
"import otter\n",
"grader = otter.Notebook(\"stats_ttest.ipynb\")"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"# What is in this assignment\n",
"\n",
"-More practice with Python's numpy and scipy libraries\n",
" -- Kmeans, stats library, random number generation, ransac/robust statistics, simple data fitting\n",
" -- Gaussian assumptions\n",
"- Data analysis\n",
"\n",
"Slides: https://docs.google.com/presentation/d/1G7nRl2zlwGrdodZyJXtsAfDU5yB7oSGVxSim8x_hzbw/edit?usp=sharing"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"# Scenario: You have a data set that you *think* clusters into two groups. \n",
"For each data point you have an associated y value; once you've split the data set into clusters you also want to find the y=f(x) value for each cluster\n",
"\n",
"- Version 1: You're given the labels (supervised learning) and you want to see if they're distinguishable\n",
"- Version 2: You are NOT given the labels, so you use k-means to get an initial labeling, and (optionally) a very simplified version of RANSAC to improve the label assignment\n",
"\n",
"## Assumptions:\n",
"- The x values for each cluster are normally distributed around the cluster's center - i.e., the noise is normally distributed\n",
"- The y values are related to the x values by a Gaussian, i.e., f is a Gaussian. Note that it could be anything else - a polynomial, etc\n",
"- There are random outliers in the data set - these points are random both in x and y\n",
"\n",
"## Analysis steps (some of these are done for you):\n",
"1) Plot the data to determine if you can at least *visually* separate the clusters. We'll do two plots - one that shows the x,y values, the second that shows the distribution of the x-values\n",
" - if the distribution of the x-values does not have two reasonably clear peaks, you can't separate the two sets of samples\n",
"\n",
"2) Version 1: Run a t-test on the known labels to determine if the two x-value distributions are distinguishable\n",
"2) Version 2: Find the x centers of the clusters using kmeans on the x values\n",
"3) CHECK that these are two distinct clusters using a t-test\n",
"4) \"Fit\" a Gaussian to each data set to get an approximation of the parameters for the f function (mu and sigma)\n",
"5) [Optional] Remove outliers (data points that are not on the Guassian) using a RANSAC algorithm\n",
"6) CHECK that there's a reasonable Gaussian fit to the two data sets\n",
"7) [Optional] do a curve fit to further improve the Gaussian reconstruction\n",
"8) Plot your results over the original data\n",
"\n",
"##   To play with: The parameters that you're given at the start should pass both checks. \n",
"For the last part of the assignment, deliberately \"break\" the t-test/reconstruction in the following ways\n",
"1) Move the centers together so that the two x distributions are not distinguishable (the histogram does not have two peaks). Does the t-test fail? What does k-means do? How \"good\" are the Gaussian fits? Is each data set a Gaussian? How close can you get before the remove_outlier code is no longer able to 'fix' the result?\n",
"2) Change just the standard deviations, the number of samples, and the noise level until the t-test breaks. What happens to the kmeans result? Can the remove_outlier code 'fix' the result? How does the added noise play into this?\n",
"3) Change the ratio of the number of samples for each peak. As the ratio moves farther and farther away from 50/50, what happens to the original t-test? What happens to the kmeans split?\n",
"4) Re-labeling and re-assignment (optional) - What happens if you re-label? What problems can it fix? Try relabeling once versus twice. Try making the gaussians more similar (moving their centers together, changing sampling ratios, increasing overlap)\n",
"5) Noise and outlier removal (optional) - What happens if you crank up the noise on the y-values? At what point does the re-labeling/outlier removal break down? What happens if you increase/decrease the aggressivenes of the outlier removal?\n",
"\n",
"## Other things to play with:\n",
"1) What happens if you set the number of clusters to 3, 4, or 5? (code will crash if more than 5)\n",
"2) What happens if you increase the number of random samples? How many can you tolerate?\n",
"\n",
"Note 1: I plot the y values with the x values, but you are doing clustering on the x values ONLY, and then fitting the y values - you're not clustering on both together\n",
"\n",
"Note 2: I've included all of the libraries/methods you should need (imports)"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"# Doing the imports for you - you should be able to do this entire assignment with these imports\n",
"import numpy as np\n",
"from numpy.random import uniform, normal\n",
"import matplotlib.pyplot as plt\n",
"from scipy.cluster.vq import kmeans2\n",
"from scipy.stats import ttest_ind\n",
"from scipy.optimize import curve_fit"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## Data generation\n",
"\n",
"I've written these for you. These two functions generate the samples/data that we'll be using for this assignment."
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"# Gaussian function\n",
"def gaussian(x, mu, sigma):\n",
"    \"\"\"Gaussian with given mean, sigma\n",
"    @param x - the input x value\n",
"    @param mu - the mean\n",
"    @param sigma - the standard deviation\n",
"    @return y = gauss(x) \"\"\"\n",
"    return (1.0 / (sigma * np.sqrt(2 * np.pi))) * np.exp(- (x - mu) ** 2 / (2 * sigma ** 2))"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"# Generate data from two normal distributions, with some random additional noise\n",
"def two_gaussians(mus, sigmas, noise_level, n_samples):\n",
"    \"\"\" Generate some random data with two peaks\n",
"    The closer the means relative to their standard deviations, plus the more noise, and the more uneven the number\n",
"    of samples, the harder it will be to tell there are two standard deviations the two distributions apart\n",
"    @param mus - centers of the two gaussians\n",
"    @param sigmas - standard deviations of the two gaussians\n",
"    @param noise_level - how much uniform noise to add to y value\n",
"    @param n_samples - Number of samples for the gaussians, and the uniform noise (triplet of integers)\n",
"    @return a 2 X sum(n_samples) array, x values in the first row, and the original labels (0, 1, -1)\n",
"    \"\"\"\n",
"    # Pre-allocate space - in this case, just easier to do this than to try to do an append/build of some sort\n",
"    samples = np.zeros((2, sum(n_samples)))\n",
"    orig_labels = np.zeros((sum(n_samples)), dtype=int)\n",
"\n",
"    # The Gaussian samples, with noise added to the y values\n",
"    n_start = 0\n",
"    for i, (mu, sigma, n) in enumerate(zip(mus, sigmas, n_samples[0:2])):\n",
"        # Get x values that are normally distributed\n",
"        samples[0, n_start:n_start+n] = normal(mu, sigma, n)\n",
"        orig_labels[n_start:n_start+n] = i\n",
"        # Evaluate Guassian at those x values and add noise based on height of gaussian\n",
"        y_noise = noise_level * gaussian(mu, mu, sigma)\n",
"        noise_y = uniform(-y_noise / 2.0, y_noise / 2.0, n)\n",
"        samples[1, n_start:n_start+n] = gaussian(samples[0, n_start:n_start+n], mu, sigma) + noise_y\n",
"        n_start += n\n",
"\n",
"    # Random noise evenly distributed across the x/y spectrum\n",
"    #   Uniformally distribute x samples across the x values from the two Gaussians\n",
"    #      and same for y\n",
"    for i in range(0, 2):\n",
"        samples[i, n_start:n_start + n_samples[2]] = uniform(np.min(samples[i, 0:n_start]),\n",
"                                                             np.max(samples[i, 0:n_start]), n_samples[2])\n",
"    orig_labels[n_start:n_start + n_samples[2]] = -1\n",
"    \n",
"    # 2xn array of samples, 1xn array of labels\n",
"    return samples, orig_labels\n"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## Plotting\n",
"I've written several plotting routines for you; you shouldn't have to change these. You SHOULD look to see what the code does/how it does it\n",
"\n",
"I always like to put plotting code into a function, because, chances are, you'll need to call it more than once at some point. Also, plotting gets really messy, so it's nice to hide the messyness."
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"def plot_samples(axs, samples, mus, sigmas, n_samples):\n",
"    \"\"\" Plot the samples and the source Gaussians\n",
"    @param axs - the plot window to draw into\n",
"    @param samples - the 2xn set of samples, divvied up by n_samples\n",
"    @param mus - the means\n",
"    @param sigmas - the standard deviations\n",
"    @param n_samples - a triplet, number of samples for each Gaussian plus the noise\"\"\"\n",
"    \n",
"    # I like using fancy colors...\n",
"    colors = ['lightcoral', 'aqua', 'magenta']\n",
"\n",
"    n_start = 0\n",
"    str_title = \"Original samples\\n\"\n",
"    for i, n in enumerate(n_samples):\n",
"        n_end = n_start + n\n",
"        axs.scatter(samples[0, n_start:n_end],\n",
"                    samples[1, n_start:n_end], c=colors[i], marker='.')\n",
"\n",
"        # The Gaussians - plot the original curves as well as the samples\n",
"        if i < 2:\n",
"            x_min = np.min(samples[0, n_start:n_end])\n",
"            x_max = np.max(samples[0, n_start:n_end])\n",
"            xs = np.linspace(x_min, x_max, 100)\n",
"            axs.plot(xs, gaussian(xs, mus[i], sigmas[i]), color=colors[i], linestyle='-')\n",
"            str_title = str_title + f\"mu {mus[i]:0.1f} sigma {sigmas[i]:0.2f}\\n\"\n",
"\n",
"        n_start += n\n",
"    str_title = str_title[0:-1]   # Take out the extra \\n\n",
"    axs.set_title(str_title)"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"def plot_density(axs, samples):\n",
"    \"\"\" Plot the density of the samples along the x axis\n",
"    @param axs - subplot window\n",
"    @param samples - the samples\"\"\"\n",
"    axs.hist(samples[0, :], bins=40)\n",
"    axs.set_title(\"X-density samples\")"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"def plot_kmeans_result(axs, centers, samples, sample_ids):\n",
"    \"\"\" Plot what kmeans returned to us\n",
"    @param axs - window to plot into\n",
"    @param centers - the centers returned by kmenas\n",
"    @param samples - the original samples,\n",
"    @param sample_ids - the cluster each sample was assigned to\"\"\"\n",
"    colors = ['lightcoral', 'aqua', 'magenta', 'grey', 'blue']\n",
"    for i in range(0, len(centers)):\n",
"        axs.scatter(samples[0, sample_ids == i], samples[1, sample_ids == i], c=colors[i], marker='.')\n",
"\n",
"    # Plot the centers afterwards so they show up on top\n",
"    for i in range(0, len(centers)):\n",
"        axs.plot(centers[i], 0, color='black', marker='x', markersize=10)\n",
"        axs.plot(centers[i], 0, color=colors[i], marker='x', markersize=8)"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"def plot_fit_results(axs, in_str_title, fitted_mus, fitted_sigmas, errs, samples, labels):\n",
"    \"\"\" Plot the samples with labels and the fitted gaussian curves\n",
"    @param axs - figure to plot in\n",
"    @param in_str_title - title to start plot with\n",
"    @param fitted_mus - the fitted means\n",
"    @param fitted_sigmas - the fitted sigmas\n",
"    @param errs - the L2 errors\n",
"    @param samples - the original samples\n",
"    @param labels - the original labels - label -1 means outlier\"\"\"\n",
"    colors = ['lightcoral', 'aqua', 'magenta', 'grey', 'blue']\n",
"\n",
"    str_title = in_str_title + \"\\n\"\n",
"    for i, (mu, sigma, err) in enumerate(zip(fitted_mus, fitted_sigmas, errs)):\n",
"        axs.scatter(samples[0, labels == i],\n",
"                    samples[1, labels == i], c=colors[i], marker='.')\n",
"\n",
"        x_min = mu - 4 * sigma\n",
"        x_max = mu + 4 * sigma\n",
"        xs = np.linspace(x_min, x_max, 100)\n",
"        axs.plot(xs, gaussian(xs, mu, sigma), color=colors[i], linestyle='-')\n",
"\n",
"        str_title = str_title + f\"mu {mu:0.1f} sigma {sigma:0.2f} err {err:0.2f}\\n\"\n",
"\n",
"    # The points not in a cluster\n",
"    axs.scatter(samples[0, labels == -1],\n",
"                samples[1, labels == -1], c=\"black\", marker='.')\n",
"\n",
"    str_title = str_title[0:-1]  # Get rid of the \\n\n",
"    axs.set_title(str_title)"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## The actual data analysis code you get to write\n",
"\n",
"Clustering, t-test, outlier removal, data fitting\n",
"- For the clustering, use kmeans2 from the scipy library (see import above)\n",
"- For the t-test, use an independent t-test from the scipy library (see import above)\n",
"-- Note: Do a bit of googling/break out the stats book to determine values to use to see if the t-test said there was a difference. There is no \"right\" answer for this, just convention\n",
"- For the outlier removal, just find data points that don't really fit either distribution. You could get really fancy here and actually determine (for the overlap region) if a data point belongs to gaussian 1 or 2...\n",
"- There's two versions of data fitting - one just gets the mean/sd from the sample points, the other actually does a curve fit, trying to match the shape of the Gaussian [optional]"
]
},
{
"cell_type": "markdown",
"metadata": {
"deletable": false,
"editable": false
},
"source": [
"## Question 1: KMeans cluster"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": [
"otter_answer_cell"
]
},
"outputs": [],
"source": [
"def cluster_kmeans(x_values, n_centers):\n",
"    \"\"\" Do k-means on the x values to find the n clusters\n",
"    @param x_values - x values from samples\n",
"    @param n_centers - number of centers to look for (2 or 3, can be up to 5)\n",
"    @returns - centers (in original coordinate system), assignment of points to centers (labels)\"\"\"\n",
"\n",
"    # TODO: Call kmeans and return the result\n",
"    ..."
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": [
"otter_answer_cell"
]
},
"outputs": [],
"source": [
"# An example of calling cluster_kmeans\n",
"ret_value = cluster_kmeans(uniform(-1, 1, 20), 2)\n",
"print(ret_value)"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"deletable": false,
"editable": false
},
"outputs": [],
"source": [
"grader.check(\"kmeans_cluster\")"
]
},
{
"cell_type": "markdown",
"metadata": {
"deletable": false,
"editable": false
},
"source": [
"## Question 2: t-test\n",
"Do t-test on two samples"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": [
"otter_answer_cell"
]
},
"outputs": [],
"source": [
"def t_test(samples_a, samples_b):\n",
"    \"\"\" Perform a t-test with the two sample sets\n",
"    @param samples_a - the samples from the first cluster\n",
"    @param samples_b - the samples from the second cluster\n",
"    @return True if dissimilar and statistically significant as a string (T/F different, t= ? p = ?) \"\"\"\n",
"\n",
"    # TODO \n",
"    # Step 1: Call ttest_ind with the two samples\n",
"    # Step 2: Convert the results to True/False\n",
"    # Step 3: Make a string that has the t and p values\n",
"    # Step 4: Return both the True/Fasle and the string\n",
"    ..."
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": [
"otter_answer_cell"
]
},
"outputs": [],
"source": [
"# Should print out: True, t=-14.7, p=0.000\n",
"res = t_test(normal(-0.2, 0.2, 100), normal(0.2, 0.2, 100))\n",
"print(res)"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"deletable": false,
"editable": false
},
"outputs": [],
"source": [
"grader.check(\"t_test\")"
]
},
{
"cell_type": "markdown",
"metadata": {
"deletable": false,
"editable": false
},
"source": [
"## Given a set of samples, calculate an estimated mean/sigma and return the error"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": [
"otter_answer_cell"
]
},
"outputs": [],
"source": [
"def fit_x_values_one_cluster(samples_x, samples_y):\n",
"    \"\"\" Given x and y values, 'fit' the x values by calculating the mean and the standard variagion\n",
"    Evaluate the fit using the y values (L2 difference between gaussian(x) and samples_y)\n",
"    @param samples_x - x samples\n",
"    @param samples_y - y samples\n",
"    @return mu, sigma, L2 err\"\"\"\n",
"\n",
"    # TODO\n",
"    # Step 1: calculate the mean and the standard deviation of the x values\n",
"    # Step 2: Use the Gaussian to generate y values from the samples_x values\n",
"    # Step 3: Sum up the difference between the samples_y and the values returned from Step 2\n",
"    # Return the mean, the standard deviation, and the err\n",
"    \n",
"    #  L2 norm - note that it makes sense (usually) to divide by the number of samples so that the numerical value\n",
"    #    is the \"same\" no matter the number of samples\n",
"\n",
"    ..."
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": [
"otter_answer_cell"
]
},
"outputs": [],
"source": [
"check_mu = 0.0\n",
"check_sigma = 0.1\n",
"xs = normal(check_mu, check_sigma, 1000)\n",
"ys = gaussian(xs, check_mu, check_sigma)\n",
"\n",
"# Should return something close to (0.0, 0.1, 0.1) += 0.001\n",
"res = fit_x_values_one_cluster(xs, ys)\n",
"print(res)"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"deletable": false,
"editable": false
},
"outputs": [],
"source": [
"grader.check(\"fit_x\")"
]
},
{
"cell_type": "markdown",
"metadata": {
"deletable": false,
"editable": false
},
"source": [
"## Fit y values as well (optional) using curve_fit\n"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": [
"otter_answer_cell"
]
},
"outputs": [],
"source": [
"def fit_y_values_one_cluster(samples_x, samples_y):\n",
"    \"\"\"Use a gradient search method to calculate a mean and a sigma that minimize the L2 norm\n",
"    @param samples_x - x samples\n",
"    @param samples_y - y samples\n",
"    @return mu, sigma, L2 err\"\"\"\n",
"    \n",
"    # TODO: \n",
"    # Step1: Fit the x values to the cluster using fit_x_values_one_cluster\n",
"    # Step 2: Fit a gaussian to the y values using curve_fit\n",
"    # Step 3: Use the mean, sd from step 2 to generate y values\n",
"    # Step 4: Compute the error (L2 norm) between the y values in step 3 and samples_y\n",
"    # Returning fit_x_values so code won't break\n",
"    return fit_x_values_one_cluster(samples_x, samples_y)\n",
"    # END PROMPT\"\"\""
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": [
"otter_answer_cell"
]
},
"outputs": [],
"source": [
"check_mu = 0.0\n",
"check_sigma = 0.1\n",
"xs = normal(check_mu, check_sigma, 1000)\n",
"ys = gaussian(xs, check_mu, check_sigma)\n",
"# Should return something close to (0.0, 0.1, 0.0001) += 0.000001\n",
"res = fit_y_values_one_cluster(xs, ys)\n",
"print(res)    "
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"deletable": false,
"editable": false
},
"outputs": [],
"source": [
"grader.check(\"optional_y_fit\")"
]
},
{
"cell_type": "markdown",
"metadata": {
"deletable": false,
"editable": false
},
"source": [
"<!-- BEGIN QUESTION -->\n",
"\n",
"## Robust statistics - removing outliers (optional)\n",
"Note - the best way to check this method is to plot and see which outliers were removed. Only far away dots should turn black"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": [
"otter_answer_cell"
]
},
"outputs": [],
"source": [
"def remove_outliers(samples, labels, n_clusters, perc_mean_err):\n",
"    \"\"\" Determine which data points you want to mark as outliers.\n",
"      The more aggressive you are, the more risk of removing valid samples.\n",
"      There are a variety of ways to determine if a point is an outlier; a simple one is to set a clip value as\n",
"      a percentage of the average noise (rather than a fixed number)\n",
"    @param samples - the original 2xn data set\n",
"    @param labels - a 1xn set of integer labels, with the label -1 indicating the data point is an outlier\n",
"    @param n_clusters - how many unique clusters there are\n",
"    @param perc_mean_err - what percentage of the mean error to clip at (should be bigger than 1)\n",
"    @returns new_labels - a 1xn set of integer labels, with the label -1 indicating the data point is an outlier\"\"\"\n",
"\n",
"    # TODO\n",
"    # Step 1 assign each data point to the best-fit Gaussian curve\n",
"    #   Doesn't matter if this is an L0 or L2 norm - just a function that is monotonically increasing with error\n",
"    #   This will be a for loop over each cluster\n",
"    # Step 2: For each sample, determine (based on clip value) if you want to label that as an outlier (-1)\n",
"    # Return the labels as a numpy array with the labels (integers)\n",
"\n",
"    # Returning labels so other code won't break\n",
"    return labels\n",
"    # END PROMPT\"\"\"\n"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": [
"otter_answer_cell"
]
},
"outputs": [],
"source": [
"# Call once - this is a syntax check, not a correct answers check\n",
"\n",
"# Generate and plot the data, colored by sample\n",
"mus = (-0.2, 0.3)\n",
"sigmas = (0.15, 0.2)\n",
"noise_level = 0.1\n",
"n_clusters = 2\n",
"n_samples=(10, 15, 2)\n",
"\n",
"samples, labels_orig = two_gaussians(mus, sigmas, noise_level, n_samples)\n",
"\n",
"# Do the clustering on just the x values\n",
"centers, labels = cluster_kmeans(samples[0], n_clusters)\n",
"\n",
"# Should be an array of 10 + 15 + 2 integers, with values -1 through n_clusters\n",
"# The first 10 values should have the same label (mostly), the next 15 should have the same value (mostly)\n",
"# and any -1s should be at the end\n",
"better_labels = remove_outliers(samples, labels_orig, n_clusters, 10.0)\n",
"print(better_labels)"
]
},
{
"cell_type": "markdown",
"metadata": {
"deletable": false,
"editable": false
},
"source": [
"<!-- END QUESTION -->\n",
"\n",
"<!-- BEGIN QUESTION -->\n",
"\n",
"## Do the example try-it tests.\n",
"Stubs are given here - change the parameter numbers to make it \"break\" as outlined at the top of this file/in the slides\n"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": [
"otter_answer_cell"
]
},
"outputs": [],
"source": [
"def fit_values_all_clusters(fit_fc, samples, labels, n_clusters):\n",
"    \"\"\" For each cluster (given by the labels) 'fit' a Gaussian\n",
"    Loop through the clusters and call fit_x_values_one_cluster\n",
"    @param fit_fc - which function to fit\n",
"    @param samples - all 2xn sample points\n",
"    @param labels - cluster labels for each sample point - -1 is an outlier\n",
"    @param n_clusters - number of clusters\n",
"    @return mus, sigmas, L2 errs\"\"\"\n",
"\n",
"    # For all clusters, \"fit\" a Gaussian and collect the error\n",
"    # Since this is basically a boring for loop, I'm giving it to you\n",
"    #   This could be done with numpy arrays, but I chose lists because we aren't doing any calculations over the\n",
"    #   arrays, and it's a bit easier to do this with a list append in python\n",
"    fitted_mus = []\n",
"    fitted_sigmas = []\n",
"    errs = []\n",
"    for i in range(0, n_clusters):\n",
"        mu, sigma, err = fit_fc(samples[0, labels == i], samples[1, labels == i])\n",
"        fitted_mus.append(mu)\n",
"        fitted_sigmas.append(sigma)\n",
"        errs.append(err)\n",
"    return fitted_mus, fitted_sigmas, errs"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": [
"otter_answer_cell"
]
},
"outputs": [],
"source": [
"# This function does the whole thing - makes the data, plots it, calls the cluster/fit functions, plots them...\n",
"def do_example(mus, sigmas, noise_level, n_samples, n_clusters, perc_mean_err_clip):\n",
"    \"\"\"Run a complete generate data, fit, plot example\n",
"    @param mus - centers of the two gaussians\n",
"    @param sigmas - standard deviations of the two gaussians\n",
"    @param noise_level - how much uniform noise to add to y value, plus random points\n",
"    @param n_samples - Number of samples for the gaussians, and the uniform noise\n",
"    @param n_clusters - Number of clusters to try\n",
"    @param perc_mean_err_clip - what percentage of the mean error to clip when doing outlier removal\n",
"    \"\"\"\n",
"    fig, axs = plt.subplots(2, 4, figsize=(16, 5))\n",
"\n",
"    # Generate and plot the data, colored by sample\n",
"    samples, labels_orig = two_gaussians(mus, sigmas, noise_level, n_samples)\n",
"    plot_samples(axs[0, 0], samples, mus, sigmas, n_samples)\n",
"    for mu, sigma in zip(mus, sigmas):\n",
"        y_noise = noise_level * gaussian(mu, mu, sigma)\n",
"        axs[0, 0].plot([mu, mu], [-y_noise / 2.0, y_noise / 2.0], 'k-')\n",
"        axs[0, 0].plot([mu - sigma, mu + sigma], [0, 0], 'k--')\n",
"\n",
"    # Histogram of all samples\n",
"    plot_density(axs[0, 1], samples)\n",
"\n",
"    # T-test with the original sample labels\n",
"    res_ttest = t_test(samples[0, labels_orig == 0], samples[0, labels_orig == 1])\n",
"    plot_kmeans_result(axs[0, 2], mus, samples, labels_orig)\n",
"    axs[0, 2].set_title(f'T-Test result\\n{res_ttest}')\n",
"\n",
"    # Do the clustering on just the x values\n",
"    centers, labels = cluster_kmeans(samples[0], n_clusters)\n",
"    #  t_test should have a string that has True or False, followed by the t-test numerical results\n",
"    res_ttest = t_test(samples[0, labels == 0], samples[0, labels == 1])\n",
"    plot_kmeans_result(axs[1, 0], centers, samples, labels)\n",
"    axs[1, 0].set_title(f'KMeans t-test result\\n{res_ttest}')\n",
"\n",
"    # For all clusters, \"fit\" a Gaussian and plot the result.\n",
"    #   Title string has the L2 error in it for each cluster\n",
"    fitted_mus, fitted_sigmas, errs = fit_values_all_clusters(fit_x_values_one_cluster, samples, labels, n_clusters)\n",
"    plot_fit_results(axs[1, 1], \"Fit x\", fitted_mus, fitted_sigmas, errs, samples, labels)\n",
"\n",
"    # Optional - remove the outliers and assign the labels based on best fit to the data\n",
"    #  Try doing this once, then see what happens when you do it twice\n",
"    better_labels = remove_outliers(samples, labels, n_clusters, 10.0)\n",
"    better_better_labels = remove_outliers(samples, better_labels, n_clusters, perc_mean_err_clip)\n",
"    fitted_mus, fitted_sigmas, errs = fit_values_all_clusters(fit_x_values_one_cluster, samples, better_better_labels, n_clusters)\n",
"    plot_fit_results(axs[1, 2], \"Fit robust\", fitted_mus, fitted_sigmas, errs, samples, better_better_labels)\n",
"\n",
"    # Optional - remove the outliers and assign the labels based on best fit to the data\n",
"    #  Try doing this once, then see what happens when you do it twice\n",
"    fitted_mus, fitted_sigmas, errs = fit_values_all_clusters(fit_y_values_one_cluster, samples, labels, n_clusters)\n",
"    plot_fit_results(axs[1, 3], \"Fit y\", fitted_mus, fitted_sigmas, errs, samples, labels)\n",
"\n",
"    plt.tight_layout()\n"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": [
"otter_answer_cell"
]
},
"outputs": [],
"source": [
"# Original\n",
"do_example(mus=(-0.2, 0.3), sigmas=(0.15, 0.2), noise_level=0.1, \n",
"               n_samples=(500, 500, 50), n_clusters=2, perc_mean_err_clip=3.0)"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": [
"otter_answer_cell"
]
},
"outputs": [],
"source": [
"# Change so means are too close together\n",
"\n",
"do_example(mus=(-0.2, 0.3), sigmas=(0.15, 0.2), noise_level=0.1, \n",
"           n_samples=(500, 500, 50), n_clusters=2, perc_mean_err_clip=3.0)"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": [
"otter_answer_cell"
]
},
"outputs": [],
"source": [
"# Change so standard deviations overlap too much/are too broad\n",
"\n",
"do_example(mus=(-0.2, 0.3), sigmas=(0.15, 0.2), noise_level=0.1, \n",
"           n_samples=(500, 500, 50), n_clusters=2, perc_mean_err_clip=3.0)"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": [
"otter_answer_cell"
]
},
"outputs": [],
"source": [
"# Change ratio of samples of the two Gaussians\n",
"\n",
"do_example(mus=(-0.2, 0.3), sigmas=(0.15, 0.2), noise_level=0.1, \n",
"           n_samples=(500, 500, 50), n_clusters=2, perc_mean_err_clip=3.0)"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": [
"otter_answer_cell"
]
},
"outputs": [],
"source": [
"# Try 3 clusters (or 1)\n",
"\n",
"do_example(mus=(-0.2, 0.3), sigmas=(0.15, 0.2), noise_level=0.1, \n",
"           n_samples=(500, 500, 50), n_clusters=2, perc_mean_err_clip=3.0)"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": [
"otter_answer_cell"
]
},
"outputs": [],
"source": [
"# Test better labeling/outlier removal\n",
"\n",
"do_example(mus=(-0.2, 0.3), sigmas=(0.15, 0.2), noise_level=0.1, \n",
"           n_samples=(500, 500, 50), n_clusters=2, perc_mean_err_clip=3.0)"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": [
"otter_answer_cell"
]
},
"outputs": [],
"source": [
"# Test outlier removal/fitting with a lot of noise\n",
"\n",
"do_example(mus=(-0.2, 0.3), sigmas=(0.15, 0.2), noise_level=0.1, \n",
"           n_samples=(500, 500, 50), n_clusters=2, perc_mean_err_clip=3.0)"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": [
"otter_answer_cell"
]
},
"outputs": [],
"source": []
},
{
"cell_type": "markdown",
"metadata": {
"deletable": false,
"editable": false
},
"source": [
"<!-- END QUESTION -->\n",
"\n",
"## Hours and collaborators\n",
"Required for every assignment - fill out before you hand-in.\n",
"\n",
"Listing names and websites helps you to document who you worked with and what internet help you received in the case of any plagiarism issues. You should list names of anyone (in class or not) who has substantially helped you with an assignment - or anyone you have *helped*. You do not need to list TAs.\n",
"\n",
"Listing hours helps us track if the assignments are too long."
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": [
"otter_answer_cell"
]
},
"outputs": [],
"source": [
"\n",
"# List of names (creates a set)\n",
"worked_with_names = {\"not filled out\"}\n",
"# List of URLS FW25(creates a set)\n",
"websites = {\"not filled out\"}\n",
"# Approximate number of hours, including lab/in-class time\n",
"hours = -1.5"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"deletable": false,
"editable": false
},
"outputs": [],
"source": [
"grader.check(\"hours_collaborators\")"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"### To submit\n",
"\n",
"- Do a restart then run all to make sure everything runs ok\n",
"- Save the file\n",
"- Submit this .ipynb file through gradescope\n",
"- Take out/suppress all extra print statements\n",
"\n",
"If the Gradescope autograder fails, please check here first for common reasons for it to fail\n",
"    https://docs.google.com/presentation/d/1tYa5oycUiG4YhXUq5vHvPOpWJ4k_xUPp2rUNIL7Q9RI/edit?usp=sharing\n",
"\n"
]
}
],
"metadata": {
"kernelspec": {
"display_name": "Python 3",
"language": "python",
"name": "python3"
},
"language_info": {
"codemirror_mode": {
"name": "ipython",
"version": 3
},
"file_extension": ".py",
"mimetype": "text/x-python",
"name": "python",
"nbconvert_exporter": "python",
"pygments_lexer": "ipython3",
"version": "3.11.7"
},
"otter": {
"OK_FORMAT": true,
"tests": {
"fit_x": {
"name": "fit_x",
"points": 2,
"suites": [
{
"cases": [
{
"code": ">>> abs(fit_x_values_one_cluster(normal(0.1, 0.2, 1000), gaussian(normal(0.1, 0.2, 1000), 0.1, 2))[0] - 0.1) < 0.1\n",
"hidden": false,
"locked": false
},
{
"code": ">>> abs(fit_x_values_one_cluster(normal(0.1, 0.2, 1000), gaussian(normal(0.1, 0.2, 1000), 0.1, 2))[1] - 0.2) < 0.1\n",
"hidden": false,
"locked": false
}
],
"scored": true,
"setup": "",
"teardown": "",
"type": "doctest"
}
]
},
"hours_collaborators": {
"name": "hours_collaborators",
"points": 1,
"suites": [
{
"cases": [
{
"code": ">>> assert not 'not filled out' in worked_with_names\n",
"hidden": false,
"locked": false
},
{
"code": ">>> assert not 'not filled out' in websites\n",
"hidden": false,
"locked": false
},
{
"code": ">>> assert hours > 0\n",
"hidden": false,
"locked": false
}
],
"scored": true,
"setup": "",
"teardown": "",
"type": "doctest"
}
]
},
"kmeans_cluster": {
"name": "kmeans_cluster",
"points": 1,
"suites": [
{
"cases": [
{
"code": ">>> assert cluster_kmeans(uniform(0, 1, 100), 2)[0].shape[0] == 2\n",
"hidden": false,
"locked": false
}
],
"scored": true,
"setup": "",
"teardown": "",
"type": "doctest"
}
]
},
"optional_y_fit": {
"name": "optional_y_fit",
"points": 0,
"suites": [
{
"cases": [
{
"code": ">>> abs(fit_y_values_one_cluster(np.linspace(0, 1, 100), gaussian(np.linspace(0, 1, 100), 0.1, 2))[2]) < 0.001\n",
"hidden": false,
"locked": false
}
],
"scored": true,
"setup": "",
"teardown": "",
"type": "doctest"
}
]
},
"t_test": {
"name": "t_test",
"points": 2,
"suites": [
{
"cases": [
{
"code": ">>> assert t_test(normal(-0.3, 0.1, 100), normal(0.1, 0.3, 100))[0:4] == 'True'\n",
"hidden": false,
"locked": false
}
],
"scored": true,
"setup": "",
"teardown": "",
"type": "doctest"
}
]
}
}
}
},
"nbformat": 4,
"nbformat_minor": 2
}