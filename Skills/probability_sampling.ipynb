{
"cells": [
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"deletable": false,
"editable": false
},
"outputs": [],
"source": [
"# Initialize Otter\n",
"import otter\n",
"grader = otter.Notebook(\"probability_sampling.ipynb\")"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"# What is in this file/notebook\n",
"\n",
"Slides: https://docs.google.com/presentation/d/1vZC32UCamhyJWJBQIuP5AXKK896mBges_eO2H3Vo3q4/edit?usp=sharing\n",
"\n",
"How do you use numpy's stats package to generalize a single random variable for:\n",
"a) Booleans (T/F)\n",
"b) Discrete variables (a, b, c)\n",
"c) Binned \"continuous\" variables - 0.1-0.2, 0.2-0.3, etc\n",
"\n",
"Think of these as functions as simulating real-world events - query the sensor for if the door is open (y/n), ask where the robot is (contiuous location OR a grid square in the world), ask which room you're in (discrete variable, kitchen dining room, etc). These are all fancy versions of a coin toss (returns T/F with 50% probability each), a roll of the dice (returns 1..6 with equal probability).\n",
"\n",
"ALL of these \"simulate probability\" routines can be implemented using JUST numpy's uniform number generator  (generates a number between 0 and 1 with all values equally likely). The simplest way to think of all of these methods is that you chop up the unit interval 0..1 into the number of possible outcomes, with each bit of the unit interval representing how likely that event is. Then you just generate a number from 0 to 1 and see which bin you fell into.\n",
"\n",
"Why the functions are set up they way they are: You need to input how likely each discrete event is. There's three basic methods for specifying this.\n",
"1) List each discrete event and how likely it is\n",
"2) All events are equally likely, just say how many there are (bins) and the mapping between the bins and the 'labels'.Usually the bins represent some spatial variable like location or angle, but could be movement\n",
"3) There is a function that represents how likely each event is, with the x coordinate representing some continuous variable like distance (think Gaussian error for movement)\n",
"\n",
"For each method that you'll implement the above information is passed in using a dictionary. I'm using a dictionary (instead of a class) because it's a bit easier to understand/implement, but the 'right' way to do this is as a class (see the last, optional, problem)."
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"outputs": [],
"source": [
"# The imports you'll need\n",
"import numpy as np\n",
"import matplotlib.pyplot as plt\n"
]
},
{
"cell_type": "markdown",
"metadata": {
"deletable": false,
"editable": false
},
"source": [
"## Boolean\n",
"Simplest random variable - returns True or False, with some probability\n",
"\n",
"Since probability of returning False is 1-prob(True), only need to specify one value.\n"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": [
"otter_answer_cell"
]
},
"outputs": [],
"source": [
"def sample_boolean_variable(info_variable):\n",
"    \"\"\" Generate one sample from a boolean variable\n",
"    @param info_variable is a dictionary containing the probability of the sensor returning True\n",
"    @returns True or False \"\"\"\n",
"\n",
"    # Probabilities have to be between 0 and 1...\n",
"    #. info_variable[\"prob_return_true\"] accesses the value stored in the dictionary\n",
"    if info_variable[\"prob_return_true\"] < 0.0 or info_variable[\"prob_return_true\"] > 1.0:\n",
"        ValueError(f\"Value {info_variable['prob_return_true']} not between zero and one\")\n",
"\n",
"    # First, use random.uniform to generate a number between 0 and one. Note that this is a uniform distribution, not\n",
"    #  a Gaussian one\n",
"    #.  random is a library in numpy (np), and uniform is a function in the random library\n",
"    #.  Google \"numpy random\"\n",
"    zero_to_one = np.random.uniform()\n",
"\n",
"    # See slides - if the random variable is below the probability of returning true, return true. Otherwise, return false\n",
"    # TODO: Return True or False\n",
"    ..."
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": [
"otter_answer_cell"
]
},
"outputs": [],
"source": [
"# First, check that you have no obvious syntax errors. This doesn't guarantee that your code is correct, just\n",
"#  that it doesn't crash and it returns True or False \n",
"boolean_variable = {\"prob_return_true\": 0.7}\n",
"ret_val = sample_boolean_variable(boolean_variable)\n",
"if ret_val is True or ret_val is False:\n",
"    print(\"sample_boolean: Passed syntax check\")\n"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": [
"otter_answer_cell"
]
},
"outputs": [],
"source": [
"# Test function: If your sample_boolean_variable is implemented correctly, then it should\n",
"#.  return \"True\" approximately test_prob_value percent of the time. This function directly\n",
"#.  checks that by, well, calling your function 10000 times and counting how many times it returned True...\n",
"def test_boolean(test_prob_value=0.6, n_samples=1000, b_print=True):\n",
"    \"\"\" Check if the sample_boolean is doing the right thing by calling it lots of times\n",
"    @param test_prob_value - any value between 0.0 and 1.0\n",
"    @param n_samples - how many samples to try. As this gets bigger, the percentage should get closer to test_prob_value\n",
"    @param b_print - whether or not to print out intermediate results\n",
"    @returns True if sample_boolean_variable is working correctly\"\"\"\n",
"\n",
"    if b_print:\n",
"        print(f\"Testing boolean with {test_prob_value} probability\")\n",
"    boolean_info_variable = {\"prob_return_true\": test_prob_value}\n",
"\n",
"    count_true = 0\n",
"    count_false = 0\n",
"    for _ in range(0, n_samples):\n",
"        if sample_boolean_variable(boolean_info_variable) == True:\n",
"            count_true += 1\n",
"        else:\n",
"            count_false += 1\n",
"\n",
"    perc_true = count_true / (count_true + count_false)\n",
"    if b_print:\n",
"        print(f\"Perc true from sampling: {perc_true}, expected {boolean_info_variable['prob_return_true']}\")\n",
"    if not np.isclose(perc_true, boolean_info_variable[\"prob_return_true\"], atol=0.05):\n",
"        if b_print:\n",
"            print(\"Failed\")\n",
"        return False\n",
"\n",
"    if b_print:\n",
"        print(\"Passed\")\n",
"    return True"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": [
"otter_answer_cell"
]
},
"outputs": [],
"source": [
"# Note: This should return true and print out passed. However, sometimes the random number generator will not be\n",
"#  your friend and it will fail - you're expecting the count to come out around 0.6 +- noise\n",
"# This makes sure you are at least using the same seed to the pseudo random number generator\n",
"np.random.seed = 5\n",
"print(f\"Boolean result: {test_boolean()}\")"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"deletable": false,
"editable": false
},
"outputs": [],
"source": [
"grader.check(\"boolean_sample\")"
]
},
{
"cell_type": "markdown",
"metadata": {
"deletable": false,
"editable": false
},
"source": [
"# Discrete\n",
"\n",
"A list of discrete variables and their corresponding likelihood.\n",
"\n",
"Because we need one value for each variable, and a name for each variable, store this as name/probabilty pair. Name is the key, probability is the value\n",
"\n",
"If you have forgotten how to do dictionaries, go do the tutorial on dictionaries before attempting this problem\n",
"\n",
"Hint: You want the .items() iterator. If you don't know what that is, or don't know what a key, value pair is, go do the tutorial."
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": [
"otter_answer_cell"
]
},
"outputs": [],
"source": [
"def sample_discrete_variable(info_variable):\n",
"    \"\"\" Generate one sample from the given discrete variable distribution\n",
"    Your code should NOT need to know what the actual keys are, how many there are, or what\n",
"    the actual values are - i.e., your code should NOT include things like\n",
"             if key == \"True\" or if z < 0.8\n",
"    @param info_variable contains pairs of values with probabilities. Probabilites should sum to one\n",
"    @returns one of the discrete values (keys) in the dictionary \"\"\"\n",
"\n",
"    # First, I'll do some checks for you\n",
"    for v in info_variable.values():\n",
"        # Probabilities have to be between 0 and 1...\n",
"        if v < 0.0 or v > 1.0:\n",
"            ValueError(f\"Value {v} not between zero and one\")\n",
"\n",
"    # And they have to sum to one\n",
"    if not np.isclose(sum(info_variable.values()), 1.0):\n",
"        ValueError(f\"Sum of probabilities should be 1, is {sum(info_variable.values())}\")\n",
"\n",
"    # Now, use random to generate a number between 0 and one\n",
"    zero_to_one = np.random.uniform()\n",
"\n",
"    # See slides - \"stack\" the probabilities - if the value lies in the discrete value's stack, return that one\n",
"    # You should use a FOR loop. If you're struggling with the FOR loop, try writing this with an if - else if -else if\n",
"    #  for JUST the test case in the next cell - it won't work for test_discrete\n",
"    #  \n",
"    # TODO - return one of the key values in the dictionary. \n",
"    ..."
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": [
"otter_answer_cell"
]
},
"outputs": [],
"source": [
"# Syntax check - does your code run and return a key?\n",
"#.  Does NOT check if your code is correct, just that it returns one of the keys\n",
"\n",
"check_discrete_tri = {\"red\": 0.2, \"green\": 0.5, \"blue\": 0.3}\n",
"\n",
"#. Call 10 times\n",
"b_passed = True\n",
"for _ in range(0, 10):\n",
"    ret_value = sample_discrete_variable(check_discrete_tri)\n",
"    if not (ret_value == \"red\" or ret_value == \"green\" or ret_value == \"blue\"):\n",
"        b_passed = False\n",
"        print(f\"Discrete: Failed syntax check, returned {ret_value}, expected red, green, or blue string\")\n",
"\n",
"if b_passed:\n",
"    print(\"Passed syntax check\")"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": [
"otter_answer_cell"
]
},
"outputs": [],
"source": [
"def test_discrete(n_samples=10000, b_print=True):\n",
"    \"\"\" Test three different random variables\n",
"    @param n_samples - number of times to sample, bigger numbers are slower but more accurate\n",
"    @param b_print - set to True if you want a lot of stuff printed out\n",
"    @return True (passed) or False (did not pass)\"\"\"\n",
"    if b_print:\n",
"        print(\"Testing discrete\")\n",
"\n",
"    # The following for loop will loop through each of these in turn. It is NOT doing them all at the\n",
"    #  same time - the first time through the for loop it will check the boolean case, the second time\n",
"    #  the red, green, blue, the third time the quad one\n",
"    check_boolean = {\"True\": 0.6, \"False\": 0.4}\n",
"    check_discrete_tri = {\"red\": 0.2, \"green\": 0.5, \"blue\": 0.3}\n",
"    check_discrete_quad = {\"kitchen\": 0.2, \"living room\": 0.3, \"dining room\": 0.3, \"bed room\": 0.2}\n",
"\n",
"    # check_variable will be check_boolean, then check_disrete_tri, then check_discrete_quad\n",
"    for check_variable in [check_boolean, check_discrete_tri, check_discrete_quad]:\n",
"        if b_print:\n",
"            print(f\"Checking dictionary: {check_variable}\")\n",
"        # For each discrete variable, set the counts to be zero; save as dictionary (rather than array/list) because\n",
"        #   the keys are strings\n",
"        counts = {}   # Empty dictionary\n",
"        for k in check_variable.keys():\n",
"            counts[k] = 0  # Add a key, 0 pair for each key\n",
"\n",
"        # 'throw the dice' (sample) multiple times, and update counts as you go\n",
"        for _ in range(0, n_samples):\n",
"            # Get a sample from the distribution - should return one of the keys\n",
"            ret_key = sample_discrete_variable(check_variable)\n",
"            # Add one to that discrete variable's count\n",
"            counts[ret_key] += 1\n",
"\n",
"        # Now compare the percentage values\n",
"        for k, v in check_variable.items():\n",
"            # How many times did I get this key?\n",
"            perc = counts[k] / n_samples\n",
"            if b_print:\n",
"                print(f\"Discrete value: {k}, got: {perc}, expected {v}\")\n",
"\n",
"            if not np.isclose(perc, v, atol=0.05):\n",
"                if b_print:\n",
"                    print(\"Failed\")\n",
"                return False\n",
"        if b_print:\n",
"            print(\" Passed\\n\")\n",
"    return True"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": [
"otter_answer_cell"
]
},
"outputs": [],
"source": [
"# Note, this is a little slow\n",
"# If you want to speed it up, decrease number of samples\n",
"np.random.seed = 10\n",
"res = test_discrete(n_samples=10000, b_print=True)\n",
"if res:\n",
"    print(f\"Passed discrete test\")\n",
"else:\n",
"    print(f\"Failed discrete test\")"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"deletable": false,
"editable": false
},
"outputs": [],
"source": [
"grader.check(\"discrete\")"
]
},
{
"cell_type": "markdown",
"metadata": {
"deletable": false,
"editable": false
},
"source": [
"# Binning\n",
"\n",
"It is common to represent continuous variables as a set of discrete values (eg, on a scale of 1 to 10, how much do you like this problem?). This is common with locations (grid up the space), images (pixels), volume levels on most digital devices, etc. This is often called *binning*, because you are binning continuous values into discrete bins.\n",
"\n",
"Creating a discrete random variable for a binned continuous variable is actually a special case of the discrete random variable - it's just that we don't need to explicitly label each discrete value. Instead the \"labels\" are the (typically) the value at the center of the bin. Rather than specifying unique labels for each bin, it's normal to just provide the start/stop boundaries and the number of divisions. The usual assumption is that all bins are equally likely; so if there are n bins, then the probability of generating any specific bin label is 1/n.\n",
"\n",
"Your solution should NOT have a loop in it - you should be able to calculate which bin **zero_to_one** lies in directly (see np.floor(x))."
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": [
"otter_answer_cell"
]
},
"outputs": [],
"source": [
"# Note: this function should work even if value is NOT between start and stop\n",
"#  If value is start, should return 0\n",
"#  If value is stop, should return 1\n",
"#. If value is halfway between start and stop, should return 0.5\n",
"#.   and so on\n",
"def convert_start_stop_to_zero_one(value : float, start : float, stop : float):\n",
"    \"\"\" This takes in a number between start and stop and returns a number between 0 and 1 \n",
"    @param value : the value to convert\n",
"    @param start : left side of the interval\n",
"    @param stop : right side of the interval\n",
"    @preturn how far value is from start to stop\"\"\"\n",
"\n",
"    # TODO return the percentage along\n",
"    ..."
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": [
"otter_answer_cell"
]
},
"outputs": [],
"source": [
"# Checking your conversion\n",
"assert np.isclose(convert_start_stop_to_zero_one(3.0, 3.0, 6.0), 0.0)\n",
"assert np.isclose(convert_start_stop_to_zero_one(6.0, 3.0, 6.0), 1.0)\n",
"assert np.isclose(convert_start_stop_to_zero_one(4.5, 3.0, 6.0), 0.5)\n",
"assert np.isclose(convert_start_stop_to_zero_one(0.0, -2.0, 2.0), 0.5)"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": [
"otter_answer_cell"
]
},
"outputs": [],
"source": [
"# Note: this function should work even if value is NOT between 0 and 1\n",
"#  If value is 0, should return start\n",
"#  If value is 1, should return stop\n",
"#. If value is 0.5, should retrub halfway between start and stop\n",
"#.   and so on\n",
"def convert_zero_one_to_start_stop(value : float, start : float, stop : float):\n",
"    \"\"\" This takes in a number between 0 and 1 and returns a number between start and stop \n",
"    @param value : the value to convert\n",
"    @param start : left side of the interval\n",
"    @param stop : right side of the interval\n",
"    @preturn how far value is from 0 to 1\"\"\"\n",
"\n",
"    # TODO return the percentage between start and stop\n",
"    ..."
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": [
"otter_answer_cell"
]
},
"outputs": [],
"source": [
"assert np.isclose(convert_zero_one_to_start_stop(0.0, 3.0, 6.0), 3.0)\n",
"assert np.isclose(convert_zero_one_to_start_stop(1.0, 3.0, 6.0), 6.0)\n",
"assert np.isclose(convert_zero_one_to_start_stop(0.5, 3.0, 6.0), 4.5)\n",
"assert np.isclose(convert_zero_one_to_start_stop(0.5, -2.0, 2.0), 0.0)"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": [
"otter_answer_cell"
]
},
"outputs": [],
"source": [
"# \n",
"def sample_bin_variable(info_variable):\n",
"    \"\"\"Sample from the discrete bin variable. The \"label\" to return is the center location of the bin \n",
"    Assumes all bins are equally likely (which is why there is not specific probability value given)\n",
"    @param info_variable - bin start and stop, number of bins. Keys: start, stop, n_bins\n",
"    @return The value (center) associated with the bin\"\"\"\n",
"\n",
"    zero_to_one = np.random.uniform()\n",
"    # TODO:\n",
"    #  Step 1: Calculate the size of each bin ON THE UNIT INTERVAL\n",
"    #  Step 2: Use np.floor to find the INDEX of the bin - do NOT use a for loop\n",
"    #  Step 3: Calculate the center of the bin with that index on the (start, stop) interval\n",
"    #.   Reminder that if you have 1 bin on the interval 0 to 1, then you would return 0.5 (the center of the bin)\n",
"    ..."
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": [
"otter_answer_cell"
]
},
"outputs": [],
"source": [
"# Checking the syntax of the call\n",
"check_bins = {\"start\": -2.0, \"stop\": 3.0, \"n_bins\": 10}\n",
"bin_loc = sample_bin_variable(check_bins)\n",
"if check_bins[\"start\"] < bin_loc < check_bins[\"stop\"]:\n",
"    print(\"bin sampling: return value is in correct range\")"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": [
"otter_answer_cell"
]
},
"outputs": [],
"source": [
"# Check returns center of bin 0 or bin 1\n",
"check_return_center = {\"start\": 0.0, \"stop\": 1.0, \"n_bins\": 2}\n",
"bin_loc = sample_bin_variable(check_return_center)\n",
"if not (np.isclose(bin_loc, 0.25) or np.isclose(bin_loc, 0.75)):\n",
"    print(f\"If two bins, the first bin goes from 0 to 0.5, the second from 0.5 to 1.0, so return either 0.25 or 0.75 (yours was {bin_loc})\")"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": [
"otter_answer_cell"
]
},
"outputs": [],
"source": [
"def test_bins(n_samples=10000, b_print=True):\n",
"    if b_print:    \n",
"        print(\"Testing bins\")\n",
"    # Provide the start and stop values, and the number of bins\n",
"    check_bins = {\"start\": -2.0, \"stop\": 3.0, \"n_bins\": 10}\n",
"\n",
"    counts = np.zeros(check_bins[\"n_bins\"])\n",
"\n",
"    bin_width = (check_bins[\"stop\"] - check_bins[\"start\"]) / check_bins[\"n_bins\"]\n",
"    for _ in range(0, n_samples):\n",
"        # Which bin location was selected?\n",
"        bin_loc = sample_bin_variable(check_bins)\n",
"\n",
"        # Convert back to the bin id\n",
"        bin = int(np.floor((bin_loc - check_bins[\"start\"]) / bin_width))\n",
"        \n",
"        # Add one to that bin count\n",
"        counts[bin] += 1\n",
"\n",
"    # All of the percentage values should be the same\n",
"    perc_expected = 1.0 / check_bins[\"n_bins\"]\n",
"    for i, count in enumerate(counts):\n",
"        perc_found = count / n_samples\n",
"        bin_loc = check_bins[\"start\"] + (i + 0.5) * bin_width\n",
"        if b_print:\n",
"            print(f\"Bin loc {bin_loc} perc {perc_found} expected close to {perc_expected}\")\n",
"\n",
"        if not np.isclose(perc_found, perc_expected, atol=0.05):\n",
"            if b_print:\n",
"                print(\"Failed\")\n",
"            return False\n",
"    if b_print:\n",
"        print(\"Passed\")\n",
"    return True"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": [
"otter_answer_cell"
]
},
"outputs": [],
"source": [
"print(f\"Bin result: {test_bins()}\")"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"deletable": false,
"editable": false
},
"outputs": [],
"source": [
"grader.check(\"bins\")"
]
},
{
"cell_type": "markdown",
"metadata": {
"deletable": false,
"editable": false
},
"source": [
"## Gaussian sampling\n",
"This is a generic Gaussian noise variable - I'm including it here because you'll need it in subsequent assignments. But it's basically \"store mu and sigma, then use those to generate noise\""
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": [
"otter_answer_cell"
]
},
"outputs": [],
"source": [
"def sample_gaussian_variable(info_variable):\n",
"    \"\"\"Return a sample from the Gaussian\n",
"    @param info_variable - mu and sigma\n",
"    @return A sample from the Gaussian\"\"\"\n",
"\n",
"    # Call random.normal here\n",
"    # TODO: Call np.random.normal here and return the number\n",
"    ..."
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": [
"otter_answer_cell"
]
},
"outputs": [],
"source": [
"# Checking syntax of call\n",
"check_gaussian = {\"mu\": 1.2, \"sigma\": 0.2}\n",
"sample = sample_gaussian_variable(check_gaussian)\n",
"print(f\"Sample value should be a number: {sample}\")"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": [
"otter_answer_cell"
]
},
"outputs": [],
"source": [
"def test_gaussian(n_samples=50000, b_print=True):\n",
"    \"\"\"Test the gaussian distribution by seeing if the mean/sd are the same\n",
"    @param b_print print out test results y/n\n",
"    \"\"\"\n",
"    if b_print:\n",
"        print(\"Testing Gaussian\")\n",
"    # Provide mu and sigma\n",
"    check_gaussian = {\"mu\": 1.2, \"sigma\": 0.2}\n",
"\n",
"    # This does the for loop \"in one line\" - read this as\n",
"    #   for _ in range()\n",
"    #       sample_gaussian...\n",
"    samples = [sample_gaussian_variable(check_gaussian) for _ in range(0, n_samples)]\n",
"\n",
"    # Should get out same mu/sigma\n",
"    samples_mean = np.mean(samples)\n",
"    samples_sigma = np.std(samples)\n",
"\n",
"    if not np.isclose(samples_mean, check_gaussian[\"mu\"], atol=0.05):\n",
"        raise ValueError(f\"Failed Gaussian, expected {check_gaussian['mu']}, got {samples_mean}\")\n",
"\n",
"    if not np.isclose(samples_sigma, check_gaussian['sigma'], atol=0.05):\n",
"        raise ValueError(f\"Failed Gaussian, expected {check_gaussian['sigma']}, got {samples_sigma}\\n\")\n",
"\n",
"    if b_print:\n",
"        print(\"Passed\\n\")\n",
"    return True"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": [
"otter_answer_cell"
]
},
"outputs": [],
"source": [
"print(f\"Gaussian result: {test_gaussian()}\")"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": [
"otter_answer_cell"
]
},
"outputs": [],
"source": [
"# Consider a time of flight distance sensor that you are \"testing\" with two distances; 2cm and 20cm. \n",
"# The standard deviation of the noise at 2cm is 1mm, and at 20cm it is 2cm. At a distance of 20cm the sensor is also\n",
"#.  biased by 3.0cm - i.e., on average, if something is 20cm away it will say it is 23.0cm away\n",
"\n",
"# Scipy stats has a function that takes in a mean and a standard deviation and returns a function that is that Gaussian distribution\n",
"from scipy.stats import norm\n",
"\n",
"\n",
"#  TODO Create plots (using np.random.normal()) that show what the expected error for the sensor at 2cm and 20cm\n",
"#.    Don't forget labels\n",
"# Note: Does it make sense to plot for distances < 0? \n",
"fig, axs = plt.subplots(1, 2)\n",
"\n",
"...\n",
"\n",
"# Example of plotting a normal function - edit this\n",
"# How to create \n",
"rv = norm(loc=0.0, scale=1.0)\n",
"\n",
"ts = np.linspace(-1.0, 1.0)\n",
"axs[0].plot(ts, rv.pdf(ts))\n",
"fig.tight_layout()"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"deletable": false,
"editable": false
},
"outputs": [],
"source": [
"grader.check(\"Gaussian\")"
]
},
{
"cell_type": "markdown",
"metadata": {
"deletable": false,
"editable": false
},
"source": [
"# Probability mass function (discrete) \n",
"\n",
"This is a more general version of the previous bin variable, with the main difference being that each bin has a different liklihood (as specified by the input function). So it's a combination of the discrete variable (using a running sum to determine which bin you fall in) and the bins (chopping up a continuous variable into bins).\n",
"\n",
"Technical note: In theory land, there is a difference between doing this as a continuous function (probability density) versus chopping it up into pieces (probability mass). You can actually do continuous functions, but it's a bit trickier and we don't need it (see for example https://www.comsol.com/blogs/sampling-random-numbers-from-probability-distribution-functions/)\n",
"\n",
"For this example we're going to use a class instead of a method because (in order to make it efficient) you want to pre-calculate a running sum from the given probabilities. It would be very expensive to do this every time you asked for a sample, like you did in the discrete problem.\n",
"\n",
"This is also a good time to do some fancy numpy array stuff, namely, using \"where\" to find the index (instead of writing your own for loop)"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": [
"otter_answer_cell"
]
},
"outputs": [],
"source": [
"class SampleProbabilityMassFunction:\n",
"    def __init__(self, in_pdf, x_range=(0.0, 1.0), n_bins=100):\n",
"        \"\"\" Given a probability mass function, what range of x to use, and the number of samples, create the running\n",
"        sum/data needed to generate random samples from that pmf\n",
"        @param in_pdf - the function representing the probability distribution\n",
"        @param x_range - min and max x values as a tuple\n",
"        @param n_bins - number of bins \"\"\"\n",
"\n",
"        # To \"keep\" a variable, use self.variable_name\n",
"        self.start = x_range[0]\n",
"        self.end = x_range[1]\n",
"        self.n_bins = n_bins\n",
"\n",
"        # TODO - Initialize correctly\n",
"        #  Where the bins start and end\n",
"        #.     Where the center of each bin is\n",
"        #  The amount of probability to put in each bin\n",
"        #.     Sample the in_pdf function at the center of the bin\n",
"        #.     Normalize the probabilities so they sum to 1\n",
"        #  The running sum\n",
"        #.     Calculate the left (and right) boundaries on the unit interval for each bin\n",
"\n",
"        # This is an example of making a numpy array of size n_bins (filled with zeros)\n",
"        self.bin_centers = np.zeros(shape=(n_bins))\n",
"        \n",
"        # Create the pmf by evaluating in_pdf at the center of each bin\n",
"        #   Don't forget to normalize - the sum of self.bin_heights should be 1\n",
"\n",
"        # Running sum of probabilities - bin_sum[i] = sum(bin_heights[0:i])\n",
"        #  Note: It's a bit easier to generate_sample if you make this array n_bins+1, with the first value being 0\n",
"        #   and the last value being 1         \n",
"\n",
"    def get_bin_center(self, indx : int):\n",
"        \"\"\" Return the center of bin indx (i.e., in our 2 bin on the unit interval example, \n",
"        0 would return 0.25 and 1 would return 0.75)\n",
"        @param indx : which bin\n",
"        @return bin center\"\"\"\n",
"        # TODO Return the bin center\n",
"        ...\n",
"\n",
"    def get_bin_probability(self, indx : int):\n",
"        \"\"\" Return the probability of that bin being selected\n",
"        @param indx : which bin\n",
"        @return probability\"\"\"\n",
"        # TODO return the probability\n",
"        ...\n",
"    \n",
"    def generate_sample(self):\n",
"        \"\"\" Draw one sample from the pmf\n",
"        Very similar to the discrete example above, for picking which bin, except you've pre-calculated the running sum.\n",
"        Very similar to bin_sample for returning the bin center, exept you've pre-calculated the bin centers\n",
"        @return bin center \"\"\"\n",
"        zero_to_one = np.random.uniform()\n",
"\n",
"        # You want the index i where bin_sum[i] <= zero_to_one < bin_sum[i+1]\n",
"        # Not fancy version: Use a for loop\n",
"        # Fancy version: Use np.where - see tutorial on where\n",
"        # TODO - return correct bin center               \n",
"        ...\n",
"\n",
"    def _generate_counts(self, n_samples):\n",
"        \"\"\" Generate n samples\n",
"        @param n_samples - number of samples per bin\n",
"        @returns a numpy array with the counts for each bin, normalized\"\"\"\n",
"\n",
"        # Counts\n",
"        counts = np.zeros(self.n_bins)\n",
"\n",
"\n",
"        #. call generate_sample many times\n",
"        for _ in range(0, self.n_bins * n_samples):\n",
"            x_value = self.generate_sample()\n",
"            # Should be between 0 and n_bins - 1\n",
"            assert x_value > self.start and x_value < self.end\n",
"            # Convert x value to 0, 1\n",
"            x_value_zero_one = convert_start_stop_to_zero_one(x_value, self.start, self.end)\n",
"            # convert 0, 1 value to n_bins\n",
"            x_value_bin_indx = x_value_zero_one * self.n_bins\n",
"            bin_index = np.floor(x_value_bin_indx)\n",
"            counts[int(bin_index)] += 1.0\n",
"\n",
"        # Normalize\n",
"        counts = counts / sum(counts)\n",
"        return counts\n",
"\n",
"    def test_self(self, in_pdf, b_print=True):\n",
"        \"\"\" Check/test function\n",
"        @param in_pdf - the pdf function used to generate the values\n",
"        @returns True/False\"\"\"\n",
"\n",
"        # Expected probability values\n",
"        bin_centers = np.zeros((self.n_bins))\n",
"        for indx in range(0, self.n_bins):\n",
"            bin_centers[indx]= self.get_bin_center(indx)\n",
"\n",
"        # Evaluate the function in_pdf at the bin centers \n",
"        expected_probs = in_pdf(self.bin_centers)\n",
"        \n",
"        # Normalize\n",
"        expected_probs /= np.sum(expected_probs)\n",
"\n",
"        # Call the generate_sample method many, many times\n",
"        counts = self._generate_counts(n_samples=100)\n",
"\n",
"        # This cute trick loops over expected probabilities and counts at the same time\n",
"        #.   requires that the two be arrays of the same size\n",
"        for exp, c in zip(expected_probs, counts):\n",
"            if b_print:\n",
"                print(f\"pmf perc {c} expected {exp}\")\n",
"\n",
"            if np.abs(exp - c) > 0.1:\n",
"                print(\"Failed\")\n",
"                return False\n",
"\n",
"        if b_print:\n",
"            print(\"Passed\")\n",
"        return True\n"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": [
"otter_answer_cell"
]
},
"outputs": [],
"source": [
"def pdf(x):\n",
"    \"\"\" Made-up pdf (a quadratic). Can be anything, as long as it's not negative\n",
"    @param x\n",
"    @ return (x+1) * (x+1) + 0.1\"\"\"\n",
"    return (x+1) ** 2 + 0.1"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": [
"otter_answer_cell"
]
},
"outputs": [],
"source": [
"# Syntax check\n",
"x_min = -2.0\n",
"x_max = 1.0\n",
"n_bins = 10\n",
"\n",
"# Make the class. Reminder, if you change the class, you need to re-execute the above cell\n",
"my_sample = SampleProbabilityMassFunction(pdf, (x_min, x_max), n_bins)\n",
"# Generate a sample\n",
"ret_value = my_sample.generate_sample()\n",
"if x_min < ret_value < x_max:\n",
"    print(\"PMF: Passed syntax check\")"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": [
"otter_answer_cell"
]
},
"outputs": [],
"source": [
"# TODO: Write some more test code to see if your class is doing the right thing..."
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": [
"otter_answer_cell"
]
},
"outputs": [],
"source": [
"# Now check that it actually returns the correct counts\n",
"my_pmf = SampleProbabilityMassFunction(in_pdf=pdf, x_range=(-2.0, 1.0), n_bins=10)\n",
"\n",
"# Call the test function on the class\n",
"assert my_pmf.test_self(in_pdf=pdf)"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": [
"otter_answer_cell"
]
},
"outputs": [],
"source": [
"# This is just to help you visualize what is going on/what you're trying to do\n",
"fig_2, axs_2 = plt.subplots(1, 3)\n",
"\n",
"# First, plot the original, continuous pdf NORMALIZED\n",
"ts_pdf = np.linspace(my_sample.start, my_sample.end, 4 * my_sample.n_bins)\n",
"ys_pdf = pdf(ts_pdf)\n",
"axs_2[0].plot(ts_pdf, ys_pdf, '-k', label=\"pdf\")\n",
"axs_2[0].set_title(\"Original PDF\\n(no normalization)\")\n",
"axs_2[0].legend()\n",
"\n",
"# Now do the normalized plot and your pmf\n",
"ys_pdf_normalized = ys_pdf / np.sum(ys_pdf)\n",
"axs_2[1].plot(ts_pdf, ys_pdf_normalized , '-k', label=\"normalized pdf\")\n",
"\n",
"check_bin_centers = np.zeros((my_sample.n_bins))\n",
"check_bin_values = np.zeros((my_sample.n_bins))\n",
"for indx in range(0, my_sample.n_bins):\n",
"    check_bin_centers[indx] = my_sample.get_bin_center(indx)\n",
"    check_bin_values[indx] = my_sample.get_bin_probability(indx)\n",
"axs_2[1].plot(check_bin_centers, check_bin_values, 'gX', label=\"PMF\")\n",
"axs_2[1].set_title(\"Normalized PDF\\n and discretized PDF\")\n",
"axs_2[1].legend()\n",
"\n",
"# Reconstructed counts\n",
"counts = my_sample._generate_counts(n_samples=100)\n",
"axs_2[2].set_title(\"pdf to pmf\")\n",
"\n",
"axs_2[2].plot(check_bin_centers, check_bin_values , '-k', label=\"PMF\")\n",
"axs_2[2].plot(check_bin_centers, counts, 'ob', label=\"Sampled PMF\")\n",
"axs_2[2].legend()\n",
"\n",
"print(f\"Should both be 1.0: {np.sum(ys_pdf_normalized)}, {np.sum(check_bin_values)}\")\n",
"fig_2.tight_layout()"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"deletable": false,
"editable": false
},
"outputs": [],
"source": [
"grader.check(\"pmf\")"
]
},
{
"cell_type": "markdown",
"metadata": {
"deletable": false,
"editable": false
},
"source": [
"## Hours and collaborators\n",
"Required for every assignment - fill out before you hand-in.\n",
"\n",
"Listing names and websites helps you to document who you worked with and what internet help you received in the case of any plagiarism issues. You should list names of anyone (in class or not) who has substantially helped you with an assignment - or anyone you have *helped*. You do not need to list TAs.\n",
"\n",
"Listing hours helps us track if the assignments are too long."
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"tags": [
"otter_answer_cell"
]
},
"outputs": [],
"source": [
"\n",
"# TODO - set to correct value               \n",
"# List of names (creates a set)\n",
"worked_with_names = {\"not filled out\"}\n",
"# List of URLS FW25(creates a set)\n",
"websites = {\"not filled out\"}\n",
"# Approximate number of hours, including lab/in-class time\n",
"hours = -1.5\n"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {
"deletable": false,
"editable": false
},
"outputs": [],
"source": [
"grader.check(\"hours_collaborators\")"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"### To submit\n",
"\n",
"- Do a restart then run all to make sure everything runs ok\n",
"- Save the file\n",
"- Submit this .ipynb file through gradescope, Homework 1\n",
"- Take out/suppress all print statements\n",
"\n",
"If the Gradescope autograder fails, please check here first for common reasons for it to fail\n",
"    https://docs.google.com/presentation/d/1tYa5oycUiG4YhXUq5vHvPOpWJ4k_xUPp2rUNIL7Q9RI/edit?usp=sharing\n",
"\n"
]
}
],
"metadata": {
"kernelspec": {
"display_name": "base",
"language": "python",
"name": "python3"
},
"language_info": {
"codemirror_mode": {
"name": "ipython",
"version": 3
},
"file_extension": ".py",
"mimetype": "text/x-python",
"name": "python",
"nbconvert_exporter": "python",
"pygments_lexer": "ipython3",
"version": "3.11.7"
},
"otter": {
"OK_FORMAT": true,
"tests": {
"Gaussian": {
"name": "Gaussian",
"points": 1,
"suites": [
{
"cases": [
{
"code": ">>> assert test_gaussian(b_print=False)\n",
"hidden": false,
"locked": false
}
],
"scored": true,
"setup": "",
"teardown": "",
"type": "doctest"
}
]
},
"bins": {
"name": "bins",
"points": 2,
"suites": [
{
"cases": [
{
"code": ">>> assert test_bins(b_print=False)\n",
"hidden": false,
"locked": false
}
],
"scored": true,
"setup": "",
"teardown": "",
"type": "doctest"
}
]
},
"boolean_sample": {
"name": "boolean_sample",
"points": 1,
"suites": [
{
"cases": [
{
"code": ">>> assert test_boolean(test_prob_value=0.3, n_samples=10000, b_print=False)\n",
"hidden": false,
"locked": false
}
],
"scored": true,
"setup": "",
"teardown": "",
"type": "doctest"
}
]
},
"discrete": {
"name": "discrete",
"points": 2,
"suites": [
{
"cases": [
{
"code": ">>> assert test_discrete(b_print=False)\n",
"hidden": false,
"locked": false
}
],
"scored": true,
"setup": "",
"teardown": "",
"type": "doctest"
}
]
},
"hours_collaborators": {
"name": "hours_collaborators",
"points": 1,
"suites": [
{
"cases": [
{
"code": ">>> assert not 'not filled out' in worked_with_names\n",
"hidden": false,
"locked": false
},
{
"code": ">>> assert not 'not filled out' in websites\n",
"hidden": false,
"locked": false
},
{
"code": ">>> assert hours > 0\n",
"hidden": false,
"locked": false
}
],
"scored": true,
"setup": "",
"teardown": "",
"type": "doctest"
}
]
},
"pmf": {
"name": "pmf",
"points": 5,
"suites": [
{
"cases": [
{
"code": ">>> assert my_sample.test_self(in_pdf=pdf, b_print=False)\n",
"hidden": false,
"locked": false
},
{
"code": ">>> assert np.isclose(np.sum(check_bin_values), 1.0)\n",
"hidden": false,
"locked": false
}
],
"scored": true,
"setup": "",
"teardown": "",
"type": "doctest"
}
]
}
}
}
},
"nbformat": 4,
"nbformat_minor": 2
}