{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lec_act_1_data_structures.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture goals\n",
    "\n",
    "1. Understand the benefit of numpy (over lists) for operating over lists of numbers\n",
    "2. Introduction to numpy-style array operations (slicing, mean)\n",
    "3. Dictionaries for data encapsulation\n",
    "4. Debugging strategies: Showing Variables in the variable window, interpretting errors\n",
    "\n",
    "Some \"how tos\" for Jupyter notebooks/autograder if you're starting here.\n",
    "\n",
    "- Put the cursor in a cell and hit shift-return to execute the cell. You can also click on the triangle to the left of the cell\n",
    "- Each problem has a \"grading\" cell. This is, essentially, the tests the auto-grader will run\n",
    "- If you see triple dots ... or the word \"pass\" this is short-hand for \"put your code here\". Python will ignore these, but you should delete them as you complete problems\n",
    "- You can add as many variables and cells as you want, but don't change the names of the variables given to you. The autograder is expecting those names\n",
    "- When you think everything is working, hit Restart and Run All (buttons at the top). This will make sure the code you turn in is the same code Gradescope runs. We do check for this; look at the code cells - they have a number next to them. If you've done a Restart and Run all those should start from 1\n",
    "- If you click on the colored bar to the left of the cell (or output) this hides the contents of the cell. Try it to hide this cell and bring it back\n",
    "\n",
    "Lecture activity hand-ins for in-person class: Get as much done as you can before Wednesday class, but if you get stuck, just stop and bring your questions to class. \n",
    "\n",
    "Every assignment has additional instruction slides. They are listed as \"Slides\" below, and can also be reached from Canvas. There's also a video that walks through what you do where. \n",
    "\n",
    "Resources:\n",
    "- Slides: https://docs.google.com/presentation/d/1lVYGqoStt0ZdnRAYMfF9Km6f0NgMNkuYgINsRhXASwI/edit?usp=sharing\n",
    "- Short and sweet video (see Canvas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access all numpy functions as np.\n",
    "import numpy as np\n",
    "\n",
    "# Note, if this fails try running week_0_python/a_start_here_libraries.ipynb first\n",
    "#  If you've done that, then make sure that the kernel you've selected for both is the same\n",
    "#  If that doesn't work, find a TA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Practice question\n",
    "\n",
    "This is a modification of one of the problems in **b_practice_numpy.ipynb**. If you're stuck, go back to the tutorial and practice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Make a 100x200 size numpy array with random numbers between -1 and 1\n",
    "# Find the min, max, and mean of the data\n",
    "# Count how many of those numbers are negative\n",
    "# Count how many of those numbers are not negative (>= 0)\n",
    "\n",
    "# https://numpy.org/doc/stable/reference/random/generator.html#simple-random-data\n",
    "\n",
    "# Example code on how to call random, which generates random numbers in the range 0 to 1\n",
    "my_example = np.random.random(size=(10, 10))\n",
    "\n",
    "# You might find numpy's count_nonzero useful here \n",
    "\n",
    "\n",
    "# TODO Make a 100 x 200 array of numbers between 0 and 1 using the example above (numpy.random.random function)\n",
    "my_made_up_np_array = ...\n",
    "# TODO Use arithmatic to change numbers in the range 0 to 1 to be in the range -1 to 1 instead\n",
    "#     Hint: Multiply by two and subtract by 1...\n",
    "# TODO Find the min of the array\n",
    "made_up_min = ...\n",
    "# TODO Find the max of the\n",
    "made_up_max = ...\n",
    "# TODO Count negative < 0 and not negative >= 0 values\n",
    "made_up_count_negative = ...\n",
    "made_up_count_positive = ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO Write some test functions for yourself\n",
    "# Check that the array size is correct (my_made_up_np_array.shape should be 100 x 200)\n",
    "\n",
    "# I'll do this one for you - check that the min is close to -1.0\n",
    "assert np.isclose(made_up_min, -1.0, atol=0.001)\n",
    "\n",
    "# TODO Check that the max is close to 1.0\n",
    "\n",
    "# TODO check that the sum of negative and not negative values is the size of the array (100 X 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"practice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Question 1: Stats on a list\n",
    "\n",
    "## calculate stats on a list\n",
    "\n",
    "TODO: Given a list of numbers (as a list) \n",
    "- Calculate the mean of the negative and positive values\n",
    "- Count the total number of negative/positive values\n",
    "- Store the values in a dictionary\n",
    "\n",
    "This is (mostly) just practice with a **for** loop and an **if** statement. And to see an example of the format of the problems for this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Data used for this assignment\n",
    "#   For just this week we're going to write \"in-line\" code - i.e., all the code is in one long list of code. Starting\n",
    "#   next week we'll break code up into functions so we can re-use it/change the data. For this assignment, I'm \"hard coding\"\n",
    "#   the \"data\" in this variable\n",
    "\n",
    "test_list_one = [-0.75, -0.25, 1.0 / 3.0, 2.0 / 3.0, 3.0 / 3.0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "## EXAMPLE CODE\n",
    "# Cells labeled EXAMPLE CODE have code in them that you should understand before you start the problem. Usually, they'll be\n",
    "#  code that you'll want to copy and edit for the TODO in the main problem, along with some explanations of what the code\n",
    "#  is and how it works.\n",
    "\n",
    "# Loop over a list and print out whether the element is positive or negative\n",
    "for item in test_list_one:\n",
    "    if item < 0:\n",
    "        # see tutorial on strings for the syntax of f\"\"\n",
    "        print(f\"Item {item} is negative\")\n",
    "    elif item > 0:\n",
    "        print(f\"Item {item} is positive\")\n",
    "    else:\n",
    "        print(f\"Item {item} is zero\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# SCRATCH CELL\n",
    "# Scratch cells are for you to use to try something out - usually a simpler version of the problem. These are not\n",
    "#  graded, although make sure they execute properly \n",
    "\n",
    "# Suggestion: If doing the full positive-negative split is too complicated, try writing a for loop that just\n",
    "#  loops over all of the items in test_list_one and adds them up and counts how many there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# These are the stats you will be calculating. This is more elegant/useful than creating four variables - it keeps all\n",
    "#  of the values in the same place and assigns a meaningful label (key) to them\n",
    "# See the tutorial on dictionaries on how to set/get key-value pairs in a dictionary\n",
    "dict_save_stats = {\"Mean positive\": 0.0, \"Mean negative\": -0.0, \"Count positive\": 0, \"Count negative\": 0}\n",
    "\n",
    "# TODO: \n",
    "#   Calculate the means and the counts of test_list_one and store them in the dictionary with the keys given above in dict_save_stats\n",
    "#   You'll need a for loop to go over the list and an if statement to separate into positive and negative\n",
    "# Step 1: Copy the for loop from the example code above (try writing it without looking at it first)\n",
    "#  This has the code structure you'll need, but doesn't calculate any stats.\n",
    "# Step 2: Use the dictionary dict_save_stats with the appropriate key to count the number of positives/negatives\n",
    "#    Two options: Create a count variable and set the dictionary entry to the count variable after the for loop\n",
    "#    Use the dictionary entry as the count variable (foo[\"\"] = foo[\"\"] + 1)\n",
    "# Step 3: In the loop add the positive/negative values to the appropriate dictionary entry\n",
    "# Step 4: Don't forget to divide by the count to create the mean\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Test code for list\n",
    "\n",
    "TODO: \n",
    "- Fill in the for loop above\n",
    "- Run the cell below - it will print out if your values are incorrect\n",
    "\n",
    "Next week we'll put the for loop code in a function so that we run it with other data than test_list_data. In the meantime, this is an example of writing test code to check that your code is correct. In this case, you can just look at test_list_data and see what the right answer should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Tests\n",
    "b_tests_passed = True\n",
    "if not np.isclose(dict_save_stats[\"Mean positive\"], 2.0 / 3.0):\n",
    "    b_tests_passed = False\n",
    "    print(f\"Mean positive is not correct, should be {2.0/3.0}, got {dict_save_stats['Mean positive']}\")\n",
    "\n",
    "if not np.isclose(dict_save_stats[\"Mean negative\"], -0.5):\n",
    "    b_tests_passed = False\n",
    "    print(f\"Mean negative is not correct, should be -0.5, got {dict_save_stats['Mean negative']}\")\n",
    "\n",
    "# != is not equals\n",
    "if dict_save_stats[\"Count positive\"] != 3:\n",
    "    b_tests_passed = False\n",
    "    print(f\"Count positive numbers, should be 3, got {dict_save_stats['Count positive']}\")\n",
    "\n",
    "if dict_save_stats[\"Count negative\"] != 2:\n",
    "    b_tests_passed = False\n",
    "    print(f\"Count positive numbers, should be 2, got {dict_save_stats['Count negative']}\")\n",
    "\n",
    "if b_tests_passed:\n",
    "    print(\"All array tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## A note on the autograder. \n",
    "\n",
    "As tempting as it might be to just write the number in to make the test work (instead of calculating it) you will get a zero for doing so. I.e., do not just put 2/3 into the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Question 2: Doing it again with a numpy array\n",
    "\n",
    "TODO: Same as the previous question, but this time do it for a numpy array\n",
    "- NO **if** statements or **for** loops - do this all with numpy operations\n",
    "\n",
    "You might find \"count_nonzero\" useful.\n",
    "\n",
    "As before, test code is below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Data for this problem - this takes the list and converts it to a numpy array\n",
    "# Use the variable window (click on variables above) to see the difference between test_list_one and\n",
    "#  test_nparray - they should have the same values, they're just stored differently\n",
    "test_nparray = np.array(test_list_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# EXAMPLE CODE\n",
    "# Numpy has built-in functions to do most of what you want to do\n",
    "#   this essentially does a for loop over the array and looks for the min\n",
    "example_min = np.min(test_nparray)\n",
    "print(f\"Min is {example_min}\")\n",
    "\n",
    "# But what if you want to do the if < 0 part? This is where boolean indexing comes in.\n",
    "#   Look at this variable in the variable window - this is another numpy array, but this time the array is full of \n",
    "#   False and True - it is True where the corresponding element in test_nparray is negative\n",
    "b_is_negative = test_nparray < 0\n",
    "\n",
    "# You can use this boolean array to get just the elements in the original list that were negative\n",
    "all_negative = test_nparray[b_is_negative]\n",
    "print(f\"Negative elements {all_negative}\")\n",
    "\n",
    "# or use another numpy method to count the number of non-zero - note, False is zero and True is non-zero\n",
    "#  Notice that this time the call to np is inside of the print statement - you can always take a piece\n",
    "#  of code and assign it to a variable.\n",
    "#     Try doing my_count = np.count_nonzero(b_is_negative)\n",
    "# If you get syntax errors, break the code up this way to find what part of the code is \"broken\"\n",
    "print(f\"Count negative {np.count_nonzero(b_is_negative)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Calculate these stats for test_nparray. The answers should be the same as the ones above.\n",
    "#   Do NOT just set the values - you must calculate them\n",
    "dict_save_stats_np = {\"Mean positive\": 0, \"Mean negative\": 0, \"Count positive\": 0, \"Count negative\": 0}\n",
    "\n",
    "# TODO: Calculate the mean for the positive and the negative values\n",
    "#.  Also count the number of each\n",
    "#   Do NOT use a for loop - use boolean indexing (see example above)\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# SELF TESTS\n",
    "#  Use this cell to write any additional test code for yourself. For example, you could copy the tests from the list version\n",
    "#  and use them here, just check the values in dict_save_stats_np instead of dict_save_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"nparray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Queston 3: Fix, please -  Uh oh, it doesn't work\n",
    "\n",
    "[Note: there's a visual  explanation of this numpy array in the slides listed at the top of the file]\n",
    "\n",
    "TODO: Each of the following cells had code that is \"broken\" - either it generates a syntax error OR it doesn't do what the comment says it does. TODO Fix what's broken so the grader tests pass.\n",
    "\n",
    "TODO: Read carefully through the next cell. It sets up the data you'll be using for these problems, and also has example array slicing you'll need to fix the broken bits\n",
    "\n",
    "Reminder: If you don't understand a complex piece of code, you can always break it apart. For example:\n",
    "\n",
    "**min_xy = np.min(test_data[0,1:3])**\n",
    "\n",
    "Can be broken up into two lines by creating another variable to hold the slice result\n",
    "\n",
    "**sliced_data = test_data[0,1:3]**\n",
    "\n",
    "**min_xy = np.min(sliced_data)**\n",
    "\n",
    "This makes it a lot easier to see if, for example, sliced_data is actually the slice you want (by printing it out or looking at it in the variable window)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Making a data set to practice with before lab/homework\n",
    "#  This is a modified version of the data set we'll work with\n",
    "#  It consists of x, y, z data for 10 time steps for 5 samples\n",
    "#  The x data is all between 0 and 1, the y data 0 and -1, z data 10-20\n",
    "#  The last column is 1 if the sample is good, 0 if it is bad\n",
    "#  The data is stored in a 5 x [3 * 10 + 1] array\n",
    "#  Each row (one row for each sample) looks like this\n",
    "#    x0 y0 z0 x1 y1 z1 .... x9 y9 z9 1 or 0\n",
    "\n",
    "# Make space for all of the data and fill it with zeros\n",
    "#   zeros takes a tuple with the data sizes - in this case we are making a 2 dimensional array\n",
    "#   with 5 columns (one for each sample) and 10 x,y,z value (30 total) and one extra column for the good/bad\n",
    "my_test_data = np.zeros((5, 3 * 10 + 1))\n",
    "\n",
    "# Fill in whether or not the sample is good. Every other one is good, the others are bad\n",
    "# Since zero is bad - and the array is all zeros - just set every other row, last column\n",
    "#   The -1 picks the last column, the 0::2 picks every other row\n",
    "#   Note: The left hand side has 3 elements, the right a single number - numpy interprets this to mean\n",
    "#     set all of those values to the single number\n",
    "my_test_data[0::2, -1] = 1\n",
    "\n",
    "# Fill in the x values for each sample with 0, 0.1... 1.0\n",
    "#  np.linspace() generates uniformly-spaced samples from start to stop\n",
    "#    You can assign values to specific parameters by name if you want\n",
    "#    This would be the same as np.linspace(0, 1.0, 10)\n",
    "# In this case, the array on the left hand side is 5 x 10, so we're going to use a loop to set each row\n",
    "#  to 0, 0.1 etc. one row at a time\n",
    "# shape is the size of the array; we want the number of rows so use .shape[0]\n",
    "x_data_for_one_row = np.linspace(start=0, stop=1.0, num=10)\n",
    "for r in range(0, my_test_data.shape[0]):\n",
    "    # loop through each row r\n",
    "    # Fill in column 0 to one before the end (don't overwrite the good/bad), skipping every 3\n",
    "    my_test_data[r, 0:-2:3] = x_data_for_one_row\n",
    "\n",
    "# Fill in the y values for each sample with random values\n",
    "#  np.random.uniform() generates random samples between the two values; unlike linsapce, you can set the size\n",
    "#   of the numpy array it returns. \n",
    "# The left side is all rows (5 - :) and every 3rd column starting at 1\n",
    "y_data_for_all_rows = np.random.uniform(-1.0, 0.0, size=(5, 10))\n",
    "my_test_data[:, 1::3] = y_data_for_all_rows\n",
    "\n",
    "# Now the z values - notice that we start at column 2 instead of 1\n",
    "my_test_data[:, 2::3] = np.random.uniform(10.0, 20.0, size=(5, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## What are the dimensions of the data?\n",
    "\n",
    "We know what the dimensions of the data are - we just made it in the cell above. Pretend for a moment that you just read **my_test_data** in from a file and you don't know how many samples there are or how many time steps. You **do** know that each sample has an x,y, and z value for each time step, and that the last column is the success/fail column\n",
    "\n",
    "Again, for these quesitons you need to calculate the value from the data, not just put the number in, except where noted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Number of samples - this is correct\n",
    "n_samples = my_test_data.shape[0]\n",
    "\n",
    "# Number of dimensions for each time step - we know this is 3, so set it to 3 - this is correct\n",
    "n_xyz = 3\n",
    "\n",
    "# FIX ME: Number of time steps\n",
    "n_time_steps = my_test_data.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Get out just the x values for sample 2\n",
    "#   Note, this is a somewhat subtle error - look at the size of the x values - it should be n_time_steps (10). Why\n",
    "#  is it not? How many columns does my_test_data actually have? What happens if you take every 3rd, starting at 0?\n",
    "# FIX ME\n",
    "x_values_sample_2 = my_test_data[1, 0::3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"fix_broken\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Hours and collaborators\n",
    "Required for every assignment - fill out before you hand-in.\n",
    "\n",
    "Listing names and websites helps you to document who you worked with and what internet help you received in the case of any plagiarism issues. You should list names of anyone (in class or not) who has substantially helped you with an assignment - or anyone you have *helped*. You do not need to list TAs.\n",
    "\n",
    "Listing hours helps us track if the assignments are too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "# List of names (creates a set)\n",
    "worked_with_names = {\"not filled out\"}\n",
    "# List of URLS 2U5 (creates a set)\n",
    "websites = {\"not filled out\"}\n",
    "# Approximate number of hours, including lab/in-class time\n",
    "hours = -1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"hours_collaborators\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To submit\n",
    "\n",
    "- Remove any print statements that print out a lot of stuff\n",
    "- Do a clear all outputs,  restart, then run all to make sure everything runs ok\n",
    "- Save the file\n",
    "- Submit this .ipynb file through gradescope, lecture activity 1 data structures. \n",
    "\n",
    "See detailed instructions here\n",
    "    https://docs.google.com/presentation/d/1tYa5oycUiG4YhXUq5vHvPOpWJ4k_xUPp2rUNIL7Q9RI/edit?usp=sharing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "fix_broken": {
     "name": "fix_broken",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert n_samples == 5\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert n_xyz == 3\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert n_time_steps == 10\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert x_values_sample_2.size == 10\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert np.any(np.isclose(x_values_sample_2, np.linspace(0, 1, 10)))\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "hours_collaborators": {
     "name": "hours_collaborators",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert not 'not filled out' in worked_with_names\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert not 'not filled out' in websites\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert hours > 0\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "list": {
     "name": "list",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert np.isclose(dict_save_stats['Mean positive'], 2.0 / 3.0)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert np.isclose(dict_save_stats['Mean negative'], -0.5)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert np.isclose(dict_save_stats['Count positive'], 3)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert np.isclose(dict_save_stats['Count negative'], 2)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "nparray": {
     "name": "nparray",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert np.isclose(dict_save_stats_np['Mean positive'], 2.0 / 3.0)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert np.isclose(dict_save_stats_np['Mean negative'], -0.5)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert np.isclose(dict_save_stats_np['Count positive'], 3)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert np.isclose(dict_save_stats_np['Count negative'], 2)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "practice": {
     "name": "practice",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert my_made_up_np_array.shape == (100, 200)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert np.isclose(made_up_min, -1.0, atol=0.001)\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert made_up_count_negative + made_up_count_positive == my_made_up_np_array.size\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert np.isclose(made_up_max, 1.0, atol=0.001)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
